{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from solardatatools import DataHandler\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from dask import delayed, compute, config\n",
    "from dask.distributed import Client, LocalCluster, performance_report\n",
    "import json\n",
    "import click\n",
    "import tempfile\n",
    "from solardatatools.dataio import load_cassandra_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_csv_to_dh(path):\n",
    "    \"\"\"\n",
    "    Converts a s3 CSV file into a solar-data-tools DataHandler.\n",
    "    Parameters:\n",
    "    - file: Path to the CSV file.\n",
    "    Returns:\n",
    "    - A tuple of the file name and its corresponding DataHandler.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    # Convert index from int to datetime object\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    dh = DataHandler(df)\n",
    "    name = path.split('/')[-1].removesuffix('.csv')\n",
    "    return (name, dh)\n",
    "\n",
    "\n",
    "def get_csvs_in_s3(s3, bucket, prefix):\n",
    "    \"\"\"\n",
    "    Gets the csvs in an s3 directory.\n",
    "    Parameters:\n",
    "    - s3: Boto3 s3 client\n",
    "    - bucket: Bucket containing the csvs.\n",
    "    - prefix: Prefix appended to the bucket name when searching for files\n",
    "    Returns:\n",
    "    - An array of the csv file paths.\n",
    "    \"\"\"\n",
    "    csvs = []\n",
    "    data_bucket = s3.Bucket(bucket)\n",
    "    for object_summary in data_bucket.objects.filter(Prefix=prefix):\n",
    "        if object_summary.key.endswith('.csv'):\n",
    "            file = f\"s3://{bucket}/{object_summary.key}\"\n",
    "            csvs.append(file)\n",
    "    return csvs\n",
    "\n",
    "\n",
    "def local_csv_to_dh(file):\n",
    "    \"\"\"\n",
    "    Converts a local CSV file into a solar-data-tools DataHandler.\n",
    "    Parameters:\n",
    "    - file: Path to the CSV file.\n",
    "    Returns:\n",
    "    - A tuple of the file name and its corresponding DataHandler.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file, index_col=0)\n",
    "    # Convert index from int to datetime object\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    dh = DataHandler(df)\n",
    "    name = os.path.basename(file)\n",
    "    return (name, dh)\n",
    "\n",
    "\n",
    "def get_csvs_in_dir(folder_path):\n",
    "    \"\"\"\n",
    "    Gets the csvs in a directory.\n",
    "    Parameters:\n",
    "    - folder_path: Directory containing the csvs.\n",
    "    Returns:\n",
    "    - An array of the csv file paths.\n",
    "    \"\"\"\n",
    "    csvs = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            if filename.endswith('.csv'):\n",
    "                csvs.append(file_path)\n",
    "    return csvs\n",
    "\n",
    "def run_job(data_result, track_times):\n",
    "    \"\"\"\n",
    "    Processes a single unit of data using DataHandler.\n",
    "    Parameters:\n",
    "    - data_result: Tuple of the file name and its corresponding DataHandler.\n",
    "    - track_times: Boolean to determine whether run times are added to the \n",
    "                   output.\n",
    "    Returns:\n",
    "    - A dictionary containing the name of the data and the processed report.\n",
    "    If there was an error with processing, only the name of the data is \n",
    "    returned.\n",
    "    \"\"\"\n",
    "    name = data_result[0]\n",
    "    data_handler = data_result[1]\n",
    "\n",
    "    \n",
    "    \n",
    "    try:    \n",
    "        data_handler.run_pipeline()\n",
    "        report = data_handler.report(verbose=False, return_values=True)\n",
    "        report[\"name\"] = name\n",
    "        if track_times:\n",
    "            report[\"total_time\"] = data_handler.total_time\n",
    "    except:\n",
    "        report = {}\n",
    "        report[\"name\"] = name\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a672fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_local(filename, track_times=True):\n",
    "    \"\"\"\n",
    "    Generate the analysis task for a given local file. \n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Name of the local file\n",
    "    - track_times: Booleans to determine whether run times are added \n",
    "    to the output\n",
    "\n",
    "    Returns:\n",
    "    - A Dask delayed task object for the data analysis, which depends\n",
    "    on the ingest task. \n",
    "    \"\"\"\n",
    "    task_ingest = delayed(local_csv_to_dh)(filename)\n",
    "    task_analyze = delayed(run_job)(task_ingest, track_times)\n",
    "    return task_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d56f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tasks_directory(directory, track_times=True):\n",
    "    \"\"\"\n",
    "    Generate the analysis tasks for a given directory containing csv's. \n",
    "    \n",
    "    Parameters:\n",
    "    - directory: Path of the directory containing csv's\n",
    "    - track_times: Booleans to determine whether run times are added \n",
    "    to the output\n",
    "\n",
    "    Returns:\n",
    "    - A list of Dask delayed task objects for the data analysis, \n",
    "    each of which depends on an ingest task. \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for filename in get_csvs_in_dir(directory):\n",
    "        result.append(generate_task_local(filename))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4ea5e",
   "metadata": {},
   "source": [
    "# Visualize task graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5111d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = generate_tasks_directory(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that with the graphic engine on certain machines, \n",
    "# Dask will only allow to produce one graph per code block,\n",
    "# hence the code below. \n",
    "obj_list[0].visualize() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f78239",
   "metadata": {},
   "source": [
    "# Execute Task Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a928f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_tasks(task_list):\n",
    "    \"\"\"\n",
    "    Execute a list of tasks. \n",
    "    \n",
    "    NOTE: The Dask cluster should be \n",
    "    intialized before calling this function. \n",
    "    \n",
    "    Parameters:\n",
    "    - task_list: A list of dask delayed object\n",
    "\n",
    "    Returns:\n",
    "    - A list of reports from execution\n",
    "    \"\"\"\n",
    "    reports = compute(*task_list,)\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker=4, n_workers=2)\n",
    "\n",
    "execute_tasks(obj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecabc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we need functions to generate tasks from remote sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69833093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remote_file_to_dh(file):\n",
    "    \"\"\"\n",
    "    Converts a file of remote database site into a list solar-data-tools DataHandler.\n",
    "    Parameters:\n",
    "    - site: remote IP address of database.\n",
    "    Returns:\n",
    "    - A list of tuples of the unique identifier and its corresponding DataHandler.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for site in file:\n",
    "        df = load_cassandra_data(site)\n",
    "        dh = DataHandler(df, convert_to_ts=True)\n",
    "        dh.data_frame_raw.index = dh.data_frame_raw.index.view(\"int\")\n",
    "        dh_keys = dh.keys\n",
    "        for key in dh_keys:\n",
    "            system = key[0][1]\n",
    "            system = system.strip()\n",
    "            result.append((system, dh))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26dc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_task_remote(file, track_times=True):\n",
    "    \"\"\"\n",
    "    Generate the analysis task for a given system.\n",
    "\n",
    "    Parameters:\n",
    "    - system: Name of the system\n",
    "    - track_times: Booleans to determine whether run times are added\n",
    "    to the output\n",
    "\n",
    "    Returns:\n",
    "    - A Dask delayed task object for the data analysis, which depends\n",
    "    on the ingest task.\n",
    "    \"\"\"\n",
    "    task_analyze = []\n",
    "    task_ingest = remote_file_to_dh(file)\n",
    "    for task in task_ingest:\n",
    "        task_analyze.append(delayed(run_job)(task, track_times))\n",
    "    return task_analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71975716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tasks_remote_database(db_list):\n",
    "    \"\"\"\n",
    "    Generate the analysis tasks for remote database.\n",
    "\n",
    "    Parameters:\n",
    "    - db_list: Path of the directory containing a list of sites from remote database\n",
    "\n",
    "    Returns:\n",
    "    - A list of Dask delayed task objects for the data analysis,\n",
    "    each of which depends on an ingest task.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    with open(db_list, \"r\") as file:\n",
    "        result.extend(generate_task_remote(file))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90620016",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = generate_tasks_remote_database(\"/Users/cclin/Documents/14798-SLAC Apache Beam Parallel Processing on AWS /resources/db_list\")\n",
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44bc451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
